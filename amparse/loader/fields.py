# -*- coding: utf-8 -*-
# Copyright 2022 by Hitachi, Ltd.
# All rights reserved.

import logging
from typing import Dict, List, Tuple
import statistics
import copy
import numpy as np
import torch
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer, PreTrainedTokenizerFast

from amparse.common import params as P
from amparse.common import util as util
from amparse.loader.corpus import Corpus, Sentence
from amparse.loader.vocab import AMVocab


def tokenize_input(
        input_text: str,
        tokenizer: PreTrainedTokenizerFast,
        max_encode: int = 4096,
        do_lower_case: bool = False,
) -> Dict:
    """
    Tokenize the input text, get masks and sentence-wise results

    Parameters
    ----------
    input_text : str
        The input text
    tokenizer : PreTrainedTokenizerFast
        The tokenizer instance
    max_encode : int
        The max sequence length
    do_lower_case : bool
        Whether to lower the characters

    Returns
    ----------
    res : Dict
        The tokenization results
    """
    # Tokenizer encode
    res_tokenize = tokenizer.encode_plus(
        input_text.lower() if do_lower_case else input_text,
        return_offsets_mapping=True,
        add_special_tokens=True
    )

    # Get offset mapping of the tokens
    token_ids = []
    tokens = []
    offsets = []
    for offset, token_id in zip(copy.deepcopy(res_tokenize['offset_mapping']), res_tokenize['input_ids']):

        if token_id != tokenizer.cls_token_id and token_id != tokenizer.sep_token_id:
            if offset[0] >= len(input_text):
                continue
            while input_text[offset[0]] == ' ':
                if offset[0] + 1 >= offset[1]:
                    break
                offset = (offset[0] + 1, offset[1])

        offsets.append(offset)
        token_ids.append(token_id)
        tokens.append(tokenizer.decode([token_id]))

    assert len(token_ids) == len(tokens) == len(offsets)

    # Get masks
    mask = torch.ones(len(token_ids), dtype=torch.bool)
    mask_special_tokens = mask.new_ones(size=mask.shape)
    if tokenizer.cls_token_id not in token_ids or tokenizer.sep_token_id not in token_ids:
        assert False, f'Invalid tokenization. ' \
                      f'input_text: "{input_text}", ' \
                      f'token_ids: {token_ids}, ' \
                      f'offsets: {offsets}'
    if token_ids.index(tokenizer.cls_token_id) != 0:
        assert False, f'"{tokenizer.name_or_path}" is not supported because cls_token_id appears in the non-zero index.'
    mask_special_tokens[token_ids.index(tokenizer.cls_token_id)] = False
    mask_special_tokens[token_ids.index(tokenizer.sep_token_id)] = False

    # Get sentence-wise inputs
    sentences = util.split_into_n_sized_chunks(token_ids, max_encode)
    sentences = [torch.tensor(s) for s in sentences]
    sentence_masks = [s.ge(-100) for s in sentences]  # True masks
    sentence_global_masks = [
        create_global_mask(iterable=s) if i == 0 else s.new_zeros(size=s.shape)
        for i, s in enumerate(sentences)
    ]
    sentences = [
        torch.nn.functional.pad(
            input=s,
            pad=[0, max_encode - s.shape[0]],
            mode='constant',
            value=tokenizer.pad_token_id,
        )
        for s in sentences
    ]
    sentence_masks = [
        torch.nn.functional.pad(
            input=sm,
            pad=[0, max_encode - sm.shape[0]],
            mode='constant',
            value=0,
        )
        for sm in sentence_masks
    ]
    sentence_global_masks = [
        torch.nn.functional.pad(
            input=sgm,
            pad=[0, max_encode - sgm.shape[0]],
            mode='constant',
            value=0,
        )
        for sgm in sentence_global_masks
    ]

    return {
        'token_ids': torch.tensor(token_ids),
        'tokens': tokens,
        'mask': mask,
        'offset_mapping': offsets,
        'mask_special_tokens': mask_special_tokens,
        'sentences': sentences,
        'sentence_masks': sentence_masks,
        'sentence_global_masks': sentence_global_masks,
    }


def create_bio_labels(subword_offsets: List[Tuple or List], spans: List[Dict], body: str) -> Tuple[str, List[str]]:
    """
    Get the BIO label sequence for component span identification

    Parameters
    ----------
    subword_offsets : List[Tuple or List]
        The subowrd offsets generated by the tokenizer
    spans : List[Dict]
        The span (including "from" and "to" attributes) of the components
    body : str
        The origin input text

    Returns
    ----------
    res : str
        Returns "Success" or "Fail" according to the span alignment
    bio : List[str]
        The BIO sequence
    """
    span_anchors = [util.anchor_span(
        spans=subword_offsets,
        span=(s['from'], s['to']),
        ignore_span=None) for s in spans
    ]

    for anc, span in zip(span_anchors, spans):
        s = (span['from'], span['to'])
        anchored_span = subword_offsets[anc[0]][0], subword_offsets[anc[1] - 1][1]

        if anchored_span != s:
            logging.warning(f'Misalignment between component span and token offsets: '
                            f'"{body[s[0]: s[1]]}" != "{body[anchored_span[0]: anchored_span[1]]}"')

    bio = [P.BI_O for _ in subword_offsets]
    for anc in span_anchors:
        frm, to = anc[0], anc[1]
        bio[frm: to] = [P.BI_I for _ in range(to - frm)]
        bio[frm] = P.BI_B

    assert len(span_anchors) == len(spans)

    if len([_ for _ in bio if _ == 'B']) != len(spans):
        logging.warning(f'Invalid span offset. subword_offsets: {subword_offsets} spans: {spans}')
        return 'Fail', [P.BI_O for _ in subword_offsets]

    return 'Success', bio


def create_global_mask(iterable) -> torch.Tensor:
    """
    Get the global mask of Longformer

    Parameters
    ----------
    iterable : Iterable
        The sized input sequence

    Returns
    ----------
    mask : torch.Tensor
        The global mask (n_iterable,)
    """
    mask = torch.tensor([0 for _ in iterable], dtype=torch.long)
    mask[0] = 1  # For the <s> token
    return mask


def get_edge_train_matrix(sentence: Sentence, edge_vocab: AMVocab, pad_id: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Get the train matrix of edge links and labels

    Parameters
    ----------
    sentence : Sentence
        The sentence instance
    edge_vocab : AMVocab
        The edge vocabulary
    pad_id : int
        The padding id

    Returns
    ----------
    arc : np.ndarray
        The train matrix of edge links (and tops) (n_component + 1, n_component + 1)
    rel : np.ndarray
        The train matrix of edge labels (and tops) (n_component + 1, n_component + 1)
    """
    nodes = sentence.data['nodes']
    nids = [n['id'] for n in nodes]
    top_nids = sentence.data['tops']
    n_nodes = len(nodes) + 1  # +1 is for the last element for the imaginary root node

    # Edge links
    arc = np.zeros(shape=(n_nodes, n_nodes), dtype='f')
    for i, edge in enumerate(sentence.data['edges']):
        assert edge['source'] == nids.index(edge['source'])
        assert edge['target'] == nids.index(edge['target'])
        # src-to-trg link
        src = nids.index(edge['source'])
        trg = nids.index(edge['target'])
        arc[src, trg] = 1.
    # Imaginary edge links for tops
    for nid in top_nids:
        # top_node-to-root link (imaginary)
        arc[-1, nid] = 1.

    # Edge labels
    rel = np.zeros(shape=(n_nodes, n_nodes), dtype='f')
    rel.fill(pad_id)
    for i, edge in enumerate(sentence.data['edges']):
        # src-to-trg label
        src = nids.index(edge['source'])
        trg = nids.index(edge['target'])
        rel[src, trg] = edge_vocab.token2id(edge['label'])
    # Imaginary edge labels for tops
    for nid in top_nids:
        # top_node-to-root label (imaginary)
        rel[-1, nid] = edge_vocab.token2id(P.TOP)

    return arc, rel


def get_dummy_edge_train_matrix(pad_id: int) -> Tuple[np.ndarray, np.ndarray]:
    # Edges
    arc = np.zeros(shape=(1, 1), dtype='f')
    # Edge labels
    rel = np.zeros(shape=(1, 1), dtype='f')
    rel.fill(pad_id)
    return arc, rel


def prepare_sentence(sentence: Sentence) -> Sentence:
    """
    Sort mrp elements (nodes, edges and tops) by anchor and re-assign ids

    Parameters
    ----------
    sentence : Sentence
        The input sentence instance

    Returns
    ----------
    sentence : Sentence
        The output sentence instance
    """
    sentence = copy.deepcopy(sentence)

    # Reassign node ids to make the id starts with zero
    nodes = sorted(sentence.data['nodes'], key=lambda x: x['anchors'][0]['from'])
    nid2newid = {n['id']: i for i, n in enumerate(nodes)}

    # Reassign node ids
    for node in sentence.data['nodes']:
        node['id'] = nid2newid[node['id']]

    # Reassign edge source and target ids
    for edge in sentence.data['edges']:
        edge['source'] = nid2newid[edge['source']]
        edge['target'] = nid2newid[edge['target']]

    # Reasign top ids
    sentence.data['tops'] = [nid2newid[t] for t in sentence.data['tops']]

    # Sort by ids
    sentence.data['tops'] = sorted(sentence.data['tops'])
    sentence.data['nodes'] = sorted(sentence.data['nodes'], key=lambda x: x['id'])
    sentence.data['edges'] = sorted(sentence.data['edges'], key=lambda x: (x['source'], x['target']))

    return sentence


class AMBaseField(object):
    """
    Base field class
    """
    def __init__(self, name: str, **kwargs):
        self.name = name
        pass

    def __repr__(self):
        pass

    def build(self, corpus, **kwargs):
        pass

    def numericalize(self, corpus, **kwargs):
        pass

    def batchfy(self, device: str, batch, **kwargs):
        pass


class InputTargetField(AMBaseField):
    """
    Field class for the input sentence and target labels
    """
    def __init__(self, name: str, model_name_or_path: str, do_lower_case: bool, max_encode: int = 4096, **kwargs):
        super().__init__(name=name, **kwargs)

        # Set params
        self.model_name_or_path = model_name_or_path
        self.do_lower_case = do_lower_case
        self.max_encode = max_encode

        # Load the tokenizer
        tokenizer = self._load_tokenizer()
        self.vocab: AMVocab = AMVocab(
            counter=None,
            min_freq=0,
            special_token2id={
                tokenizer.cls_token: tokenizer.cls_token_id,
                tokenizer.sep_token: tokenizer.sep_token_id,
                tokenizer.pad_token: tokenizer.pad_token_id,
            }
        )
        self.tokenizer: PreTrainedTokenizerFast = tokenizer

        # Vocabularies
        self.fw_vocab: AMVocab = AMVocab()
        self.fw_2_edge_vocab: Dict[str, AMVocab] = dict()
        self.fw_2_proposition_vocab: Dict[str, AMVocab] = dict()
        self.bio_vocab: AMVocab = AMVocab()
        self.vocab.extend_with_dict(d={P.PAD: -100})

    def __repr__(self):
        s = f'({self.name}) = {self.__class__.__name__} (\n'
        s += f'\ttokenizer: {str(self.tokenizer.__class__)}\n'
        s += f'\tdo_lower_case: {str(self.do_lower_case)}\n'
        s += f'\tfw_list: {str(self.fw_vocab.tokens)}\n'
        s += f'\tvocab: {str(self.vocab)}\n'
        s += f'\tbio_vocab: {str(self.bio_vocab)}\n'
        s += f'\tfw_2_proposition_vocab:\n'
        for fw, v in self.fw_2_proposition_vocab.items():
            s += f'\t\t{fw}: {str(v)}\n'
        s += f'\tfw_2_edge_vocab:\n'
        for fw, v in self.fw_2_edge_vocab.items():
            s += f'\t\t{fw}: {str(v)}\n'
        s += f')'
        return s

    def __getstate__(self):
        state = self.__dict__.copy()
        # Prevent pickling the tokenizer
        del state["tokenizer"]
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        # When loading from the pickle, we re-load the tokenizer
        self.tokenizer = self._load_tokenizer()

    def _load_tokenizer(self):
        """
        Load tokenizer from local path for remote URL
        """
        tokenizer: PreTrainedTokenizerFast = util.from_pretrained(
            AutoTokenizer,
            pretrained_model_name_or_path=self.model_name_or_path,
            do_lower_case=self.do_lower_case,
        )
        return tokenizer

    def build(self, corpus, **kwargs):
        """
        Build labels from the input corpus

        Parameters
        ----------
        corpus : Corpus
            The corpus instance
        """
        # Get labels
        fws = set()
        fw_2_edge_labels = dict()
        fw_2_proposition_labels = dict()
        for sentence in corpus.sentences:
            # Get framework label
            fw = sentence.framework
            fws.add(fw)

            # Get edge labels
            if fw not in fw_2_edge_labels:
                fw_2_edge_labels[fw] = set()
            for edge in sentence.data['edges']:
                fw_2_edge_labels[fw].add(edge['label'])

            # Get component labels
            if fw not in fw_2_proposition_labels:
                fw_2_proposition_labels[fw] = set()
            for node in sentence.data['nodes']:
                fw_2_proposition_labels[fw].add(node['label'])

        # Build framework vocab
        self.fw_vocab.extend_with_dict(d={t: i for i, t in enumerate(sorted(list(fws)))})

        # Build edge vocab
        for fw, edge_labels in fw_2_edge_labels.items():
            edge_labels.add(P.TOP)  # We add the top-relation class
            self.fw_2_edge_vocab[fw] = AMVocab()
            self.fw_2_edge_vocab[fw].extend_with_dict(d={t: i for i, t in enumerate(sorted(list(edge_labels)))})

        # Build proposition vocab
        for fw, prop_labels in fw_2_proposition_labels.items():
            self.fw_2_proposition_vocab[fw] = AMVocab()
            self.fw_2_proposition_vocab[fw].extend_with_dict(d={t: i for i, t in enumerate(sorted(list(prop_labels)))})

        # Build bio vocab for span anchors
        self.bio_vocab.extend_with_dict({P.BI_B: 0, P.BI_I: 1, P.BI_O: 2})
        return

    def numericalize(self, corpus: Corpus, **kwargs) -> List[Dict]:
        """
        Get numericalized (i.e., encoded) inputs and target labels for each sentence

        Parameters
        ----------
        corpus : Corpus
            The corpus instance

        Returns
        ----------
        numericalized_sentences : List[Dict]
            The numericalized sentences. That is, len(numericalized) == len(corpus)
        """
        numericalized_sentences = []
        seq_lens = []

        # Check if replacing the new-line is needed
        newlines = self.tokenizer.tokenize('\n')
        replace_newline = newlines == ['‚ñÅ'] or newlines == []
        if replace_newline:
            logging.info(f'Replacing newline tokens with "_" token.')

        # Get numericalized sentences
        for sentence in corpus.sentences:
            # Sort mrp elements by node anchors to ensure correct label encodings
            sentence = prepare_sentence(sentence=sentence)

            fw = sentence.framework
            inp = sentence.data['input']

            # Replace the new-line if needed
            if replace_newline:
                inp = inp.replace('\n', '_')

            # Tokenize the input text into subwords
            tokenized = tokenize_input(
                input_text=inp,
                tokenizer=self.tokenizer,
                max_encode=self.max_encode,
                do_lower_case=self.do_lower_case,
            )

            # Create the global mask for Longformer
            global_mask = create_global_mask(iterable=tokenized['offset_mapping'])

            # Obtain span anchors and component labels
            spans = []
            labels = []
            prev_offset = -1
            for i_node, node in enumerate(sentence.data['nodes']):
                assert i_node == node['id']
                anc = node['anchors'][0]
                assert prev_offset <= anc['from']
                spans.append(anc)
                labels.append(node['label'])
                prev_offset = anc['to']

            # Create the numericalized labels for span identification by span alignment for the subwords
            offsets_without_cls_sep = [so for so in tokenized['offset_mapping'] if so != (0, 0)]
            bio_res, bio = create_bio_labels(subword_offsets=offsets_without_cls_sep, spans=spans, body=inp)
            bio_ids = [self.bio_vocab.token2id(l) for l in bio]

            if bio_res == 'Success':
                # Create the numericalized component labels
                proposition_ids = [self.fw_2_proposition_vocab[fw].token2id(l) for l in labels]
                # Create the numericalized edge matrices
                edge_arcs, edge_rels = get_edge_train_matrix(
                    sentence=sentence, edge_vocab=self.fw_2_edge_vocab[fw], pad_id=self.vocab.token2id(P.PAD))
            elif bio_res == 'Fail':
                # If the span alignment failed, we use dummy labels
                logging.warning(f'Removing training labels for {sentence.id} because of the span alignment failed.')
                proposition_ids = []
                edge_arcs, edge_rels = get_dummy_edge_train_matrix(pad_id=self.vocab.token2id(P.PAD))
            else:
                assert False

            seq_lens.append(len(tokenized['token_ids']))

            # Add the numericalized sentence
            numericalized_sentences.append({
                'token_ids': tokenized['token_ids'],
                'sentences': tokenized['sentences'],
                'sentence_masks': tokenized['sentence_masks'],
                'sentence_global_masks': tokenized['sentence_global_masks'],
                'tokens': tokenized['tokens'],
                'mask': tokenized['mask'],
                'global_mask': global_mask,
                'offset_mapping': offsets_without_cls_sep,
                'mask_special_tokens': tokenized['mask_special_tokens'],
                'bio_ids': torch.tensor(bio_ids, dtype=torch.long),
                'proposition_ids': torch.tensor(proposition_ids, dtype=torch.long),
                'edge_arcs': torch.tensor(edge_arcs, dtype=torch.float),
                'edge_rels': torch.tensor(edge_rels, dtype=torch.long),
                'fw': fw,
            })

        assert len(numericalized_sentences) == len(corpus)

        # Show statistics for the length of input sequence
        if numericalized_sentences:
            logging.info(f'Built numerics for {corpus}')
            #logging.info(f'Numecalized sample of {self.__class__.__name__}: {pprint.pformat(numericalized_sentences[0])}')
            logging.info(f'\tMin token_len: {min(seq_lens)}')
            logging.info(f'\tMax token_len: {max(seq_lens)}')
            if len(seq_lens) >= 2:
                logging.info(f'\tMean token_len: {statistics.mean(seq_lens)}')
                logging.info(f'\tPstdev token_len: {statistics.pstdev(seq_lens)}')

        return numericalized_sentences

    def batchfy(self, device: str, batch, **kwargs) -> Dict:
        """
        Convert into the device batch
        """
        t_dicts = {k: [] for k in batch[0].keys()}
        for b in batch:
            for k, v in b.items():
                t_dicts[k].append(v)
        batch_size = len(list(t_dicts.values())[0])
        for t_k, t_v in t_dicts.items():
            assert len(t_v) == batch_size
        batch = t_dicts

        device_batch = dict()
        for k, lst in batch.items():
            if k in ('token_ids',):
                device_batch[k] = pad_sequence(
                    lst, batch_first=True, padding_value=self.tokenizer.pad_token_id).to(device)
            elif k in ('sentences',):
                device_batch[k] = [
                    pad_sequence(l, batch_first=True, padding_value=self.tokenizer.pad_token_id).to(device)
                    for l in lst]
            elif k in ('sentence_masks', 'sentence_global_masks'):
                device_batch[k] = [
                    pad_sequence(l, batch_first=True, padding_value=False).to(device)
                    for l in lst]
            elif k in ('mask', 'global_mask', 'mask_special_tokens'):
                device_batch[k] = pad_sequence(lst, batch_first=True, padding_value=False).to(device)
            elif k in ('offset_mapping',):
                device_batch[k] = lst  # We do not add it on GPU
            elif k in ('bio_ids', 'proposition_ids',):
                device_batch[k] = pad_sequence(
                    lst, batch_first=True, padding_value=self.vocab.token2id(P.PAD)).to(device)
            elif k in ('fw', 'tokens',):
                device_batch[k] = lst  # We do not add it on GPU
            elif k in ('edge_arcs', 'edge_rels',):
                max_len = max([l.shape[1] for l in lst if len(l) != 0])
                device_batch[k] = torch.stack([
                    torch.nn.functional.pad(
                        input=v,
                        pad=[0, max_len - v.shape[1], 0, max_len - v.shape[0]],
                        mode='constant',
                        value=self.vocab.token2id(P.PAD),
                    ) for v in lst
                ]).to(device)  # (batch, seq, seq)
        return device_batch
